{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patemotter/demystifying-ai/blob/main/notebooks/session_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "#  \n",
        "  \n",
        "# Demystifying AI - Session 4\n",
        "## Transformers and Attention\n",
        "\n",
        "\n",
        "### Pate Motter, PhD  \n",
        "\n",
        "AI Performance Engineer @ Google\n",
        "\n",
        "[LinkedIn](https://www.linkedin.com/in/patemotter/) | [GitHub](https://github.com/patemotter)\n",
        "\n",
        "---\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "# About This Notebook\n",
        "\n",
        "In Session 3, we saw how RNNs process sequences step-by-step, passing a hidden state forward like a baton in a relay race. This sequential nature, while intuitive, has drawbacks:\n",
        "\n",
        "* **Long-Range Dependencies:** Information from early parts of a long sequence can get \"diluted\" or lost by the time the RNN reaches the end (the \"vanishing gradient\" problem in training). Remembering the subject of a paragraph from the first sentence when generating the last sentence is hard.\n",
        "* **Parallelization Limits:** The sequential calculation (output at step `t` depends on step `t-1`) makes it difficult to fully utilize parallel processors (like GPUs/TPUs) during training, as each step must wait for the previous one.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ohsoE05-eYmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup and Imports\n",
        "# Run this cell first to install and import necessary libraries.\n",
        "# Install plotly for interactive visualizations if needed\n",
        "# !pip install plotly numpy torch matplotlib\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import plotly.graph_objects as go\n",
        "import plotly.subplots as sp\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "import math # For positional encoding visualization\n",
        "import re # For parsing color strings\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Consistent color scheme (similar to Session 3)\n",
        "COLOR_INPUT = 'rgba(99, 110, 250, 0.7)'     # Blue\n",
        "COLOR_HIDDEN = 'rgba(239, 85, 59, 0.7)'     # Red (used for Feed-Forward)\n",
        "COLOR_OUTPUT = 'rgba(0, 204, 150, 0.7)'     # Green\n",
        "COLOR_EDGE = 'rgba(210, 210, 210, 0.8)'     # Light Gray\n",
        "COLOR_ATTENTION = 'rgba(255, 127, 14, 0.8)' # Orange (for Attention layers)\n",
        "COLOR_POS_ENC = 'rgba(148, 103, 189, 0.7)'  # Purple (for Positional Encoding)\n",
        "COLOR_ADDNORM = 'rgba(140, 86, 75, 0.6)'    # Brown (for Add & Norm)\n",
        "BACKGROUND = 'rgba(248, 248, 248, 0.95)'    # Light background\n",
        "\n",
        "\n",
        "# Choose a pre-trained model (e.g., a small BERT model)\n",
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the PyTorch model with output_attentions=True\n",
        "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
        "\n",
        "# Example input sequence\n",
        "input_sequence_text = \"The cat sat on the mat.\"\n",
        "\n",
        "# Tokenize the input sequence\n",
        "inputs = tokenizer(input_sequence_text, return_tensors=\"pt\")\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
        "\n",
        "# Run the model and get the attention outputs\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Extract the attention weights from the last layer (you can choose other layers)\n",
        "# The 'attentions' output is a tuple of attention maps, one for each layer\n",
        "attention_weights = outputs.attentions[-1]  # Get the last layer's attention"
      ],
      "metadata": {
        "id": "iVKdtAJBHrNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Transformer Solution: Parallel Processing with Attention\n",
        "\n",
        "The Transformer architecture proposed a radical shift: **process all elements of the sequence simultaneously**.\n",
        "\n",
        "**The Core Idea:** Instead of a sequential chain, the Transformer allows every element in the sequence to directly interact with every other element. This is achieved through the **Self-Attention** mechanism:\n",
        "- Look at ALL words in the sequence simultaneously\n",
        "- For each word, determine which OTHER words are most relevant to understanding it\n",
        "- Focus \"attention\" on those relevant words regardless of distance\n",
        "\n",
        "This parallel processing and direct interaction allow Transformers to better capture long-range dependencies and train much faster on parallel hardware.\n",
        "\n",
        "This solves both major problems:\n",
        "1. No information loss over distance - word #50 can directly connect to word #1\n",
        "2. Parallel processing - the entire sequence is processed simultaneously\n"
      ],
      "metadata": {
        "id": "Di58zXKKVlla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RNN vs Transformer Flow\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "\n",
        "def create_rnn_vs_transformer_viz():\n",
        "    \"\"\"\n",
        "    Create a visualization comparing RNN and Transformer approaches.\n",
        "    \"\"\"\n",
        "    # Create a sample sentence\n",
        "    sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "    words = sentence.split()\n",
        "    n_words = len(words)\n",
        "\n",
        "    # Create the figure with two subplots\n",
        "    fig = make_subplots(\n",
        "        rows=2,\n",
        "        cols=1,\n",
        "        subplot_titles=(\"RNN: Sequential Processing\", \"Transformer: Parallel Processing with Attention\"),\n",
        "        vertical_spacing=0.2,\n",
        "        row_heights=[0.4, 0.6]\n",
        "    )\n",
        "\n",
        "    # --- RNN Visualization ---\n",
        "\n",
        "    # Node positions for RNN\n",
        "    x_positions = np.arange(n_words)\n",
        "    y_rnn = np.zeros(n_words)\n",
        "\n",
        "    # Hidden state positions (slightly above the word nodes)\n",
        "    x_hidden = np.arange(n_words)\n",
        "    y_hidden = np.ones(n_words) * 0.6\n",
        "\n",
        "    # Add word nodes\n",
        "    for i, word in enumerate(words):\n",
        "        # Add the word node\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=[x_positions[i]],\n",
        "                y=[y_rnn[i]],\n",
        "                mode='markers+text',\n",
        "                marker=dict(size=30, color='rgba(0, 0, 255, 0.5)'),\n",
        "                text=[word],\n",
        "                textposition=\"bottom center\",\n",
        "                name=\"Words\" if i == 0 else None,  # Show only once in legend\n",
        "                legendgroup=\"words\",\n",
        "                hoverinfo='skip',\n",
        "                showlegend=(i == 0)\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # Add hidden state node\n",
        "        if i > 0:  # No hidden state before the first word\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=[x_hidden[i-1]],\n",
        "                    y=[y_hidden[i-1]],\n",
        "                    mode='markers',\n",
        "                    marker=dict(size=20, color='rgba(255, 0, 0, 0.5)'),\n",
        "                    name=\"Hidden States\" if i == 1 else None,\n",
        "                    legendgroup=\"hidden\",\n",
        "                    hoverinfo='text',\n",
        "                    hovertext=f\"Hidden state after word: {words[i-1]}\",\n",
        "                    showlegend=(i == 1)\n",
        "                ),\n",
        "                row=1, col=1\n",
        "            )\n",
        "\n",
        "            # Add arrows from previous hidden state to current word\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=[x_hidden[i-1], x_positions[i]],\n",
        "                    y=[y_hidden[i-1], y_rnn[i]],\n",
        "                    mode='lines',\n",
        "                    line=dict(color='rgba(100, 100, 100, 0.8)', width=2),\n",
        "                    showlegend=False\n",
        "                ),\n",
        "                row=1, col=1\n",
        "            )\n",
        "\n",
        "        # Add arrows from current word to current hidden state\n",
        "        if i < n_words - 1:  # No hidden state after the last word (for simplicity)\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=[x_positions[i], x_hidden[i]],\n",
        "                    y=[y_rnn[i], y_hidden[i]],\n",
        "                    mode='lines',\n",
        "                    line=dict(color='rgba(100, 100, 100, 0.8)', width=2),\n",
        "                    showlegend=False\n",
        "                ),\n",
        "                row=1, col=1\n",
        "            )\n",
        "\n",
        "            # Add arrows from current hidden state to next hidden state\n",
        "            if i < n_words - 2:\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(\n",
        "                        x=[x_hidden[i], x_hidden[i+1]],\n",
        "                        y=[y_hidden[i], y_hidden[i+1]],\n",
        "                        mode='lines',\n",
        "                        line=dict(color='rgba(255, 0, 0, 0.8)', width=2),\n",
        "                        showlegend=False\n",
        "                    ),\n",
        "                    row=1, col=1\n",
        "                )\n",
        "\n",
        "    # --- Transformer Visualization ---\n",
        "\n",
        "    # Node positions for Transformer visualization\n",
        "    x_trans = np.arange(n_words)\n",
        "    y_trans = np.zeros(n_words)\n",
        "\n",
        "    # Add word nodes\n",
        "    for i, word in enumerate(words):\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=[x_trans[i]],\n",
        "                y=[y_trans[i]],\n",
        "                mode='markers+text',\n",
        "                marker=dict(size=30, color='rgba(0, 0, 255, 0.5)'),\n",
        "                text=[word],\n",
        "                textposition=\"bottom center\",\n",
        "                name=word if i == 0 else None,\n",
        "                legendgroup=\"words_trans\",\n",
        "                hoverinfo='skip',\n",
        "                showlegend=False\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "    # Add attention connections\n",
        "    # Let's create example attention patterns:\n",
        "    # 1. Pronoun to referent (\"fox\" → \"it\")\n",
        "    # 2. Adjective to noun (\"quick\" → \"fox\", \"brown\" → \"fox\", \"lazy\" → \"dog\")\n",
        "    # 3. Verb to subject (\"jumps\" → \"fox\")\n",
        "    # 4. Articles to nouns (\"the\" → \"fox\", \"the\" → \"dog\")\n",
        "\n",
        "    attention_pairs = [\n",
        "        # Format: (from_word, to_word, strength, color)\n",
        "        (\"jumps\", \"fox\", 0.8, 'rgba(255, 0, 0, 0.7)'),\n",
        "        (\"quick\", \"fox\", 0.6, 'rgba(0, 255, 0, 0.7)'),\n",
        "        (\"brown\", \"fox\", 0.7, 'rgba(0, 255, 0, 0.7)'),\n",
        "        (\"lazy\", \"dog.\", 0.7, 'rgba(0, 255, 0, 0.7)'),\n",
        "        (\"over\", \"jumps\", 0.5, 'rgba(255, 165, 0, 0.7)'),\n",
        "        (\"the\", \"dog.\", 0.6, 'rgba(128, 0, 128, 0.7)')\n",
        "    ]\n",
        "\n",
        "    added_legend_items = set()\n",
        "\n",
        "    for from_word, to_word, strength, color in attention_pairs:\n",
        "        from_idx = words.index(from_word)\n",
        "        to_idx = words.index(to_word)\n",
        "\n",
        "        # Calculate control points for curved lines\n",
        "        x0, y0 = x_trans[from_idx], y_trans[from_idx]\n",
        "        x1, y1 = x_trans[to_idx], y_trans[to_idx]\n",
        "\n",
        "        # Higher arc for longer distances\n",
        "        arc_height = 0.5 + 0.1 * abs(from_idx - to_idx)\n",
        "\n",
        "        # Determine control points for quadratic Bezier curve\n",
        "        if from_idx < to_idx:\n",
        "            xcp = (x0 + x1) / 2\n",
        "            ycp = arc_height\n",
        "        else:\n",
        "            xcp = (x0 + x1) / 2\n",
        "            ycp = -arc_height\n",
        "\n",
        "        # Create points for the curve\n",
        "        t = np.linspace(0, 1, 20)\n",
        "        x_curve = (1-t)**2 * x0 + 2*(1-t)*t * xcp + t**2 * x1\n",
        "        y_curve = (1-t)**2 * y0 + 2*(1-t)*t * ycp + t**2 * y1\n",
        "\n",
        "        # Determine legend name based on relationship type\n",
        "        legend_name = None\n",
        "        relationship_type = \"\"\n",
        "\n",
        "        if from_word == \"jumps\" and to_word == \"fox\":\n",
        "            relationship_type = \"verb_subject\"\n",
        "            legend_name = \"Verb-Subject Attention\" if \"verb_subject\" not in added_legend_items else None\n",
        "        elif from_word in [\"quick\", \"brown\"] and to_word == \"fox\":\n",
        "            relationship_type = \"adj_noun\"\n",
        "            legend_name = \"Adjective-Noun Attention\" if \"adj_noun\" not in added_legend_items else None\n",
        "        elif from_word == \"lazy\" and to_word == \"dog.\":\n",
        "            relationship_type = \"adj_noun\"\n",
        "            legend_name = \"Adjective-Noun Attention\" if \"adj_noun\" not in added_legend_items else None\n",
        "        elif from_word == \"the\" and to_word == \"dog.\":\n",
        "            relationship_type = \"article_noun\"\n",
        "            legend_name = \"Article-Noun Attention\" if \"article_noun\" not in added_legend_items else None\n",
        "        elif from_word == \"over\" and to_word == \"jumps\":\n",
        "            relationship_type = \"preposition_verb\"\n",
        "            legend_name = \"Preposition-Verb Attention\" if \"preposition_verb\" not in added_legend_items else None\n",
        "\n",
        "        if legend_name:\n",
        "            added_legend_items.add(relationship_type)\n",
        "\n",
        "        # Add the curve\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=x_curve,\n",
        "                y=y_curve,\n",
        "                mode='lines',\n",
        "                line=dict(color=color, width=3 * strength),\n",
        "                name=legend_name,\n",
        "                showlegend=legend_name is not None,\n",
        "                hoverinfo='text',\n",
        "                hovertext=f\"Attention: {from_word} → {to_word} ({strength:.1f})\"\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title=\"RNN vs Transformer: Different Ways to Process Sequences\",\n",
        "        width=1000,\n",
        "        height=800,\n",
        "        legend=dict(\n",
        "            x=1.1,\n",
        "            y=0.9,\n",
        "            traceorder=\"grouped\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Update axes\n",
        "    fig.update_xaxes(showticklabels=False, row=1, col=1)\n",
        "    fig.update_yaxes(showticklabels=False, row=1, col=1)\n",
        "    fig.update_xaxes(showticklabels=False, row=2, col=1)\n",
        "    fig.update_yaxes(showticklabels=False, row=2, col=1)\n",
        "\n",
        "    # Add annotations\n",
        "    fig.add_annotation(\n",
        "        x=4, y=1.1,\n",
        "        text=\"RNNs process words sequentially, with information flowing through a chain of hidden states\",\n",
        "        showarrow=False,\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    fig.add_annotation(\n",
        "        x=4, y=1.1,\n",
        "        text=\"Transformers process all words at once, with attention connecting related words directly\",\n",
        "        showarrow=False,\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Create and display the visualization\n",
        "fig = create_rnn_vs_transformer_viz()\n",
        "fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BiWgUZCqblMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analogy of RNN and Transformer\n",
        "\n",
        "### RNN - The Lone Detective\n",
        "\n",
        "* Imagine a detective receiving reports and evidence one piece at a time over several weeks. They read each new report, trying to remember previous details and keep a running mental summary.\n",
        "\n",
        "* When investigating a clue found in Week 10, their understanding heavily relies on the reports from Week 9. To connect it back to a specific detail mentioned in a Week 1 report, they'd have to manually sift back through the entire stack of files in order, hoping they remember or spot the relevant connection. Information is processed sequentially, and early context can easily get lost or buried.\n",
        "\n",
        "### Transformer - The Detective Team:\n",
        "\n",
        "* Now, picture a detective team standing in front of a giant evidence board where all the case files, photos, witness statements, and clues are displayed at once.\n",
        "\n",
        "* To understand the significance of one specific clue (say, a peculiar muddy footprint), the lead detective points to it and asks the team, \"What's the story with this specific footprint?\"\n",
        "\n",
        "* Simultaneously, different team members quickly scan the entire board and all files, but each focuses on finding connections specifically relevant to that footprint:\n",
        "  * One checks suspect shoe sizes and types.\n",
        "  * Another checks soil analysis reports from various locations.\n",
        "  * Another scans witness statements mentioning muddy shoes or relevant locations.\n",
        "  * Another checks security logs for relevant times.\n",
        "\n",
        "* They ignore unrelated information. They quickly pull out only the pieces of evidence directly relevant to that footprint (e.g., \"This mud matches the park,\" \"Suspect X owns these shoes,\" \"Witness saw muddy shoes near the park entrance\").\n",
        "\n",
        "* The lead detective instantly sees all these targeted, relevant connections drawn from across the entire case and synthesizes them to grasp the footprint's importance. This focused, parallel search happens for every clue they analyze."
      ],
      "metadata": {
        "id": "_Z_SncnkLr6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Core Concept #1: Positional Encoding\n",
        "\n",
        "## 1.1 The Challenge of Language\n",
        "\n",
        "Human language is inherently sequential - we communicate through ordered words, and\n",
        "changing the order changes the meaning\n",
        "\n",
        "**Subject-Object Reversal:**\n",
        "- \"Dog bites man\" vs. \"Man bites dog\"\n",
        "- \"The child gave the parent a gift\" vs. \"The parent gave the child a gift\"\n",
        "\n",
        "**Modifier Scope:**\n",
        "- \"Only I saw the movie yesterday\" vs. \"I only saw the movie yesterday\"\n",
        "  (\"no one else saw it\" vs \"I did nothing else but see it\")\n",
        "\n",
        "**Negation Position:**\n",
        "- \"Everyone is not invited\" vs. \"Not everyone is invited\"\n",
        "  (\"no one is invited\" vs \"some people are invited\")\n",
        "\n",
        "**Question vs. Statement:**\n",
        "- \"Is the train arriving?\" vs. \"The train is arriving\"\n",
        "\n",
        "**Different Meanings with Same Words/Structure:**\n",
        "- \"Turn down the offer\" (reject it) vs. \"Turn down the street\" (change direction) vs. \"Turn down the volume\" (lower the sound)\n",
        "- \"Stand by your friend\" (support them) vs. \"Stand by your friend\" (physically wait near them)\n",
        "\n",
        "AI needs to process these sequences in a way that:\n",
        "1. Understands the meaning of individual words\n",
        "2. Captures how words relate to each other\n",
        "3. Preserves important information over long distances\n",
        "4. Recognizes the significance of word order\n",
        "\n",
        "Since Transformers process all words in parallel (rather than sequentially), they need a way\n",
        "to understand word order. Without this, similar but fundamentally different sentences would look nearly identical."
      ],
      "metadata": {
        "id": "fvNMBDHYUy3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 The Solution: Positional Encoding\n",
        "\n",
        "Since the Transformer processes all input tokens (e.g., words or sub-words) in parallel without inherent recurrence, it loses the natural sense of order provided by RNNs. \"The cat chased the dog\" and \"The dog chased the cat\" would look identical to the core self-attention mechanism without additional information.\n",
        "\n",
        "To solve this, Transformers inject information about the *position* of each token in the sequence. This is done using **Positional Encodings**.\n",
        "\n",
        "### How it Works\n",
        "1.  Each token in the input sequence is first converted into a vector (an **embedding**), similar to what we saw in RNNs/MLPs. This vector represents the token's meaning.\n",
        "2.  A second vector, the **positional encoding**, which depends only on the token's position in the sequence (e.g., 1st word, 2nd word, etc.), is generated.\n",
        "3.  This positional encoding vector is **added** to the token's embedding vector.\n",
        "\n",
        "The result is an enriched embedding that contains information about both the token's meaning *and* its position.\n",
        "\n",
        "### Why it Works\n",
        "Positional encodings use a clever mathematical pattern (based on sine and cosine functions)\n",
        "with important properties:\n",
        "\n",
        "1. **Each position gets a unique pattern**: Position #1 is distinct from position #2, #3, etc.\n",
        "2. **Similar positions have similar patterns**: Position #5 and #6 have more similar patterns\n",
        "   than position #5 and #100\n",
        "3. **Works for any length**: The pattern can extend to sequences longer than what the model\n",
        "   was trained on\n",
        "\n"
      ],
      "metadata": {
        "id": "oVBGsPp4Wf0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Heatmap of Positional Encodings\n",
        "# Requires 'pos_encoding', 'embedding_dimension' from previous cells\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "from IPython.display import display, Markdown # Ensure these are available\n",
        "\n",
        "# --- Assume pos_encoding is defined from previous cell ---\n",
        "# Example definition if needed:\n",
        "def get_positional_encoding(max_seq_len, d_model):\n",
        "    pos = np.arange(max_seq_len)[:, np.newaxis]; i = np.arange(d_model)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model)); angle_rads = pos * angle_rates\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2]); angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    return angle_rads\n",
        "max_sequence_length = 512\n",
        "embedding_dimension = 128\n",
        "pos_encoding = get_positional_encoding(max_sequence_length, embedding_dimension)\n",
        "# ---\n",
        "\n",
        "# --- Parameters for Heatmap ---\n",
        "num_positions_heatmap = 512  # How many positions (now columns) to show\n",
        "num_dimensions_heatmap = 128 # How many dimensions (now rows) to show\n",
        "\n",
        "# --- Create Heatmap (Transposed) ---\n",
        "# Slice the positional encoding matrix to the desired size\n",
        "pos_encoding_slice = pos_encoding[:min(num_positions_heatmap, pos_encoding.shape[0]),\n",
        "                                  :min(num_dimensions_heatmap, pos_encoding.shape[1])]\n",
        "\n",
        "# Transpose the slice so position is on the x-axis\n",
        "pos_encoding_slice_transposed = pos_encoding_slice.T\n",
        "\n",
        "fig_heatmap_transposed = px.imshow(\n",
        "    pos_encoding_slice_transposed,\n",
        "    labels=dict(x=\"Position Index\", y=\"Dimension Index\", color=\"Encoding Value\"),\n",
        "    x=np.arange(pos_encoding_slice_transposed.shape[1]), # Label position indices (original rows) correctly\n",
        "    y=np.arange(pos_encoding_slice_transposed.shape[0]), # Label dimension indices (original columns) correctly\n",
        "    title=f\"Heatmap of Positional Encodings (First {pos_encoding_slice_transposed.shape[0]} Dimensions, First {pos_encoding_slice_transposed.shape[1]} Positions)\",\n",
        "    color_continuous_scale='viridis',\n",
        "    aspect=\"auto\"\n",
        ")\n",
        "\n",
        "fig_heatmap_transposed.update_layout(height=600, width=1000)\n",
        "\n",
        "explanation_heatmap_transposed = \"\"\"\n",
        "* This **heatmap** visualizes the positional encoding values.\n",
        "* **Columns:** Represent the position in the sequence (Position 0 at the left).\n",
        "* **Rows:** Represent the dimension index within the embedding vector (Dimension 0 at the top).\n",
        "* **Color:** Indicates the encoding value (ranging from -1 to 1, see color bar).\n",
        "* **Takeaway:** Observe the distinct patterns. Horizontal bands indicate slower changing frequencies (early dimensions, top rows), while more rapid vertical changes/checkerboards indicate higher frequencies (later dimensions, bottom rows). Crucially, each column (position) has a unique pattern across the dimensions.\n",
        "\"\"\"\n",
        "display(Markdown(explanation_heatmap_transposed))\n",
        "fig_heatmap_transposed.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aNwauyFjI_rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Computing the Positional Encoding Vectors\n",
        "# Requires 'pos_encoding', 'embedding_dimension' from previous cells\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# --- Parameters ---\n",
        "dims_to_show = 30 # How many dimensions to show\n",
        "positions_to_show = [0, 2, 4] # Which position vectors to plot\n",
        "\n",
        "# --- Create Plot ---\n",
        "fig = go.Figure()\n",
        "\n",
        "# Plot selected P_pos vectors\n",
        "for pos in positions_to_show:\n",
        "    fig.add_trace(go.Scatter(x=np.arange(dims_to_show), y=pos_encoding[pos, :dims_to_show],\n",
        "                             mode='lines+markers', name=f'Positional Vector P_{pos}'))\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title=f\"Each Position Gets a Unique Vector\",\n",
        "    xaxis_title=\"Dimension Index\", yaxis_title=\"Encoding Value\",\n",
        "    yaxis=dict(range=[-1.1, 1.1]), height=600, width=1200,\n",
        "    legend=dict(\n",
        "        yanchor=\"bottom\",\n",
        "        y=0.01,\n",
        "        xanchor=\"right\",\n",
        "        x=0.98,\n",
        "    )\n",
        ")\n",
        "\n",
        "explanation = f\"\"\"\n",
        "* This plot shows the actual positional encoding vectors ({positions_to_show}) for the first few positions.\n",
        "* Each line represents the values across the first {dims_to_show} dimensions for that specific position.\n",
        "* **Takeaway:** Notice that the line shape (the vector's 'signature') is clearly different for each position. This uniqueness is essential. (These lines are like the first few rows of the heatmap).\n",
        "\"\"\"\n",
        "display(Markdown(explanation))\n",
        "fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CWNGYd3RIzbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Adding Positional Encodings to Embeddings\n",
        "\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_positional_encoding(max_seq_len, d_model):\n",
        "    pos = np.arange(max_seq_len)[:, np.newaxis]; i = np.arange(d_model)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model)); angle_rads = pos * angle_rates\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2]); angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    return angle_rads\n",
        "\n",
        "# --- Parameters ---\n",
        "max_sequence_length = 10\n",
        "embedding_dimension = 128 # Full dimension\n",
        "vis_dimension = 2 # Only visualize first 2 dims for THIS cell's plot\n",
        "\n",
        "# --- Simulate Word Embeddings (Clearer Dummy Data) ---\n",
        "E_words = np.array([[1, 1], [1, 3], [3, 1], [3, 3]])\n",
        "word_names = [\"'cat'\", \"'sat'\", \"'on'\", \"'mat'\"]\n",
        "e_word_labels = [f\"E_{name}\" for name in word_names] # Labels for blue points\n",
        "\n",
        "# --- Get Real Positional Encodings ---\n",
        "# Calculate the FULL matrix and store it as 'pos_encoding'\n",
        "pos_encoding = get_positional_encoding(max_sequence_length, embedding_dimension)\n",
        "\n",
        "# Extract the first few vectors/dimensions needed just for this cell's visualization\n",
        "P_vectors = pos_encoding[:4, :vis_dimension] # Use pos_encoding here\n",
        "pos_labels = [f\"P_{i}\" for i in range(4)] # Generate P_0, P_1, ...\n",
        "\n",
        "# --- Calculate Combined Vectors ---\n",
        "Input_vectors = E_words + P_vectors\n",
        "# Create more descriptive labels for the green points\n",
        "input_labels = [f\" {e_word_labels[i]}+{pos_labels[i]}\" for i in range(len(word_names))]\n",
        "\n",
        "# --- Create Plot ---\n",
        "fig = go.Figure()\n",
        "\n",
        "# Plot E_words\n",
        "fig.add_trace(go.Scatter(x=E_words[:, 0], y=E_words[:, 1], mode='markers+text', name='Original Word Embedding (E)',\n",
        "                         text=e_word_labels, textposition=\"middle right\", marker=dict(color='blue', size=12)))\n",
        "\n",
        "# Plot Combined Input_vectors using the new labels\n",
        "fig.add_trace(go.Scatter(x=Input_vectors[:, 0], y=Input_vectors[:, 1], mode='markers+text', name='Modified Embedding (E+P)',\n",
        "                         text=input_labels, textposition=\"middle right\", marker=dict(color='green', size=12)))\n",
        "\n",
        "# Add arrows showing the addition (E -> E+P)\n",
        "for i in range(len(Input_vectors)):\n",
        "    fig.add_annotation(ax=E_words[i, 0], ay=E_words[i, 1], x=Input_vectors[i, 0], y=Input_vectors[i, 1],\n",
        "                       xref='x', yref='y', axref='x', ayref='y',\n",
        "                       showarrow=True, arrowhead=2, arrowcolor='purple', arrowwidth=2)\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title_text=\"Combining Word Meaning (E) with Position Signal (P)\",\n",
        "    xaxis_title=\"Dim 1\", yaxis_title=\"Dim 2\",\n",
        "    legend=dict(yanchor=\"bottom\", y=0.99, xanchor=\"left\", x=0.01),\n",
        "    height=500, width=600\n",
        ")\n",
        "fig.update_yaxes(scaleanchor='x', scaleratio=1)\n",
        "\n",
        "# --- Explanation Section ---\n",
        "explanation = \"\"\"\n",
        "**Takeaway:** The final input vectors (green) incorporate both original word meaning (blue) and a position-specific signal (purple vector).\n",
        "* **Blue Points:** Represent the 'meaning' vectors (embeddings) for different words (e.g., E_'cat').\n",
        "* **Green Points:** Represent the final vectors fed into the Transformer model.\n",
        "    The label (e.g., `E_'cat'+P_0`) explicitly shows it's the sum of the word embedding (E) for 'cat' and the positional encoding for position 0 (P_0).\n",
        "* **Purple Arrows:** Show the vector addition process (adding the specific P vector).\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(explanation))\n",
        "fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Re7NK4djIpf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Core Concept #2: Attention\n",
        "\n",
        "## 2.1 The Intuition Behind Attention\n",
        "\n",
        "**Attention** is the mechanism that allows Transformers to focus on relevant information\n",
        "when processing each word.\n",
        "\n",
        "### Human Attention Analogy\n",
        "\n",
        "When you read the sentence: \"After running the marathon, she was so tired that **SHE** couldn't\n",
        "even lift **HER** water bottle,\" you naturally understand:\n",
        "- \"**SHE**\" refers to the marathon runner\n",
        "- \"**HER**\" also refers to the same person\n",
        "\n",
        "Your brain performs attention by connecting related parts of the sentence, even when they're\n",
        "separated by several words."
      ],
      "metadata": {
        "id": "OrgCVZFRUswx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Self-Attention - Letting Inputs Talk to Each Other\n",
        "The heart of the Transformer is the **Self-Attention** mechanism. It allows the model, when processing one token (e.g., a word), to look at *all other tokens* in the *same* input sequence and determine how relevant they are.\n",
        "\n",
        "This allows the model to understand context. For example, in the sentence \"The **animal** didn't cross the street because **it** was too tired,\" self-attention helps the model understand that \"**it**\" refers to \"**animal**\".\n",
        "\n",
        "\n",
        "**Analogy: Library Research**\n",
        "Imagine you're researching a topic (**Query**). You go to a library catalogue and compare your query against the keywords or titles of books (**Keys**). When you find a strong match, you retrieve the actual book content (**Value**). Self-attention works similarly.\n"
      ],
      "metadata": {
        "id": "VcK-QAGqawZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query, Key, Value (QKV)\n",
        "\n",
        "For each input token (after adding positional encoding), the model learns to generate three distinct vectors:\n",
        "1.  **Query (Q):** Represents the current token's \"question\" or what it's looking for.\n",
        " * \"I am the word 'it', who might I refer to?\"\n",
        "2.  **Key (K):** Represents a token's \"label\" or \"identifier\" used for matching.\n",
        "  * \"I am the word 'animal', this is what I represent.\"\n",
        "3.  **Value (V):** Represents the actual content or meaning of the token.\n",
        "  * \"I am the word 'animal', here's my semantic information.\"\n",
        "\n",
        "\n",
        "### The Attention Process\n",
        "\n",
        "1.  **Calculate Scores:**\n",
        "  * The Query vector of the current token is compared with the Key vectors of *all* tokens in the sequence (including itself).\n",
        "  * This comparison is typically done using a **dot product**, similar to how we saw matrix multiplication relates inputs and weights in MLPs.\n",
        "  * A higher dot product result means a better match (higher relevance).\n",
        "2.  **Normalize Scores (Softmax):** These raw scores are scaled (often divided by the square root of the key dimension) and then converted into probabilities using a **softmax** function. This ensures all scores for a given query sum to 1, representing the \"distribution of attention\".\n",
        "3.  **Weighted Sum of Values:** The softmax scores are used as weights. Each Value vector is multiplied by its corresponding attention weight, and the results are summed up. Tokens with higher attention scores contribute more of their Value (meaning) to the final output representation for the current token.\n",
        "\n",
        "This process happens *for every token* in the sequence, allowing each token to gather context from all other tokens based on learned relevanced\n",
        "\n"
      ],
      "metadata": {
        "id": "DpCSIz0EazHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Self-Attention Visualization (Improved Clarity - Layout & Legend Fixes)\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import traceback\n",
        "\n",
        "# Function definition with fixes\n",
        "def create_qkv_visualization_clearer(sentence, focus_word_index):\n",
        "    \"\"\"\n",
        "    Create a clearer interactive visualization of Query-Key-Value attention mechanism.\n",
        "    (Fixes for cutoff text, legend clutter)\n",
        "    \"\"\"\n",
        "    words = sentence.split()\n",
        "    n_words = len(words)\n",
        "    embedding_dim = 4\n",
        "    np.random.seed(42)\n",
        "    # --- IMPORTANT NOTE ---\n",
        "    # Embeddings and Weight matrices (W_q, W_k, W_v) are RANDOM.\n",
        "    # In a real Transformer, these are learned during training.\n",
        "    # Therefore, the attention patterns shown here are illustrative of the\n",
        "    # *mechanism*, not necessarily semantically meaningful for the example sentence.\n",
        "    # ---\n",
        "    embeddings = np.random.randn(n_words, embedding_dim) * 0.5\n",
        "    W_q = np.random.randn(embedding_dim, embedding_dim)\n",
        "    W_k = np.random.randn(embedding_dim, embedding_dim)\n",
        "    W_v = np.random.randn(embedding_dim, embedding_dim)\n",
        "    Q = np.matmul(embeddings, W_q)\n",
        "    K = np.matmul(embeddings, W_k)\n",
        "    V = np.matmul(embeddings, W_v)\n",
        "    query_vector = Q[focus_word_index]\n",
        "    scores = np.matmul(query_vector, K.T)\n",
        "    scaled_scores = scores / np.sqrt(embedding_dim)\n",
        "    exp_scores = np.exp(scaled_scores - np.max(scaled_scores))\n",
        "    attention_weights = exp_scores / exp_scores.sum()\n",
        "    weighted_values = V * attention_weights[:, np.newaxis]\n",
        "    output = weighted_values.sum(axis=0)\n",
        "\n",
        "    # --- Create Visualization ---\n",
        "    fig = make_subplots(\n",
        "        rows=3, cols=1,\n",
        "        subplot_titles=(\n",
        "            f\"<b>Step 1: Calculate Scores</b> (Query '<i>{words[focus_word_index]}</i>' dot Keys)\", # Slightly shortened\n",
        "            f\"<b>Step 2: Calculate Attention Weights</b> (Softmax of Scores)\", # Slightly shortened\n",
        "            f\"<b>Step 3: Combine Values using Weights</b> (Weighted Sum)\"\n",
        "        ),\n",
        "        vertical_spacing=0.18, row_heights=[0.3, 0.3, 0.4]\n",
        "    )\n",
        "    focus_color = 'rgba(220, 50, 50, 0.8)' # Red\n",
        "    other_color = 'rgba(50, 100, 200, 0.7)' # Blue\n",
        "    bar_colors = [other_color] * n_words\n",
        "    bar_colors[focus_word_index] = focus_color\n",
        "\n",
        "    # --- Plot 1: Scores ---\n",
        "    # Add trace WITHOUT name/legend entry\n",
        "    fig.add_trace(go.Bar(\n",
        "            x=words,\n",
        "            y=scaled_scores,\n",
        "            # name=\"Scaled Scores\", # REMOVED name to declutter legend\n",
        "            showlegend=False,      # Explicitly hide from legend\n",
        "            text=[f\"{s:.2f}\" for s in scaled_scores],\n",
        "            textposition=\"outside\",\n",
        "            marker_color=bar_colors,\n",
        "            hovertemplate=\"<b>Word (Key):</b> %{x}<br><b>Scaled Score:</b> %{y:.3f}<extra></extra>\"\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # --- Plot 2: Weights ---\n",
        "     # Add trace WITHOUT name/legend entry\n",
        "    fig.add_trace(go.Bar(\n",
        "            x=words,\n",
        "            y=attention_weights,\n",
        "            # name=\"Attention Weights\", # REMOVED name to declutter legend\n",
        "            showlegend=False,         # Explicitly hide from legend\n",
        "            text=[f\"{w:.2f}\" for w in attention_weights],\n",
        "            textposition=\"outside\", # Weights are 0-1, outside usually works\n",
        "            marker_color=bar_colors,\n",
        "            hovertemplate=\"<b>Word:</b> %{x}<br><b>Attention Weight:</b> %{y:.3f}<extra></extra>\"\n",
        "        ),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    # --- Plot 3: Weighted Values (Stacked) & Output ---\n",
        "    dim_labels = [f\"Dim {j+1}\" for j in range(embedding_dim)]\n",
        "    # Add weighted value bars WITHOUT legend entries\n",
        "    for i, word in enumerate(words):\n",
        "        fig.add_trace(go.Bar(\n",
        "                x=dim_labels,\n",
        "                y=weighted_values[i], # Plot weighted values\n",
        "                name=f\"Weighted V: {word}\", # Keep name for hover info\n",
        "                showlegend=False,           # Explicitly hide from legend\n",
        "                marker_color=bar_colors[i],\n",
        "                hovertemplate=(f\"<b>Word:</b> {word}<br>\" +\n",
        "                               f\"<b>Attn W:</b> {attention_weights[i]:.3f}<br>\" +\n",
        "                               f\"<b>Dim:</b> %{{x}}<br>\" +\n",
        "                               f\"<b>Weighted V:</b> %{{y:.3f}}<extra></extra>\")\n",
        "            ),\n",
        "            row=3, col=1\n",
        "        )\n",
        "    # Add Output Line/Markers - THIS WILL BE IN THE LEGEND\n",
        "    fig.add_trace(go.Scatter(\n",
        "            x=dim_labels,\n",
        "            y=output,\n",
        "            mode='markers+lines',\n",
        "            marker=dict(size=12, color='black', symbol='star'),\n",
        "            line=dict(width=4, color='black', dash='dashdot'),\n",
        "            name=\"Output (Sum of Weighted Values)\", # Keep name for legend\n",
        "            hovertemplate=(\"<b>Output Vector</b><br>\" +\n",
        "                           \"<b>Dim:</b> %{x}<br>\" +\n",
        "                           \"<b>Value:</b> %{y:.3f}<extra></extra>\")\n",
        "        ),\n",
        "        row=3, col=1\n",
        "    )\n",
        "\n",
        "    # --- Update Layout ---\n",
        "    fig.update_layout(\n",
        "        title={\n",
        "            'text': f\"<b>Self-Attention Visualization: Focus Word = '{words[focus_word_index]}'</b>\"\n",
        "                    f\"<br>(Focus Word: <span style='color:{focus_color};'>Red</span>, Other Words: <span style='color:{other_color};'>Blue</span>)\" # Color key in title\n",
        "                    f\"<br><sup><i>Note: Embeddings & Weights are random.</sup>\",\n",
        "            'x': 0.5, 'y': 0.98, 'xanchor': 'center', 'yanchor': 'top'\n",
        "        },\n",
        "        width=900, height=950,\n",
        "        barmode='stack', # Apply 'stack' globally - primarily affects Plot 3\n",
        "        bargap=0.3,\n",
        "        # showlegend=True, # Default is True, only named traces with showlegend=True appear\n",
        "        legend=dict(\n",
        "            traceorder='reversed', # Show Output first if multiple items were present\n",
        "            itemsizing='constant'  # Keep legend marker size constant\n",
        "        ),\n",
        "        hovermode='x unified'\n",
        "    )\n",
        "\n",
        "    # --- Update Axes ---\n",
        "    # Calculate dynamic range for Plot 1 y-axis to prevent cutoff text\n",
        "    min_score = np.min(scaled_scores)\n",
        "    max_score = np.max(scaled_scores)\n",
        "    score_range = max_score - min_score\n",
        "    # Add buffer (e.g., 15% of range, or a minimum buffer if range is tiny)\n",
        "    y_buffer_plot1 = max(score_range * 0.15, 0.1)\n",
        "    y_range_plot1 = [min_score - y_buffer_plot1, max_score + y_buffer_plot1]\n",
        "\n",
        "    fig.update_yaxes(title_text=\"Scaled Score\", range=y_range_plot1, row=1, col=1) # Apply calculated range\n",
        "    fig.update_yaxes(title_text=\"Attention Weight\", range=[0, 1.05], row=2, col=1) # Range 0-1, small buffer for text\n",
        "    fig.update_yaxes(title_text=\"Weighted Value\", row=3, col=1) # Auto-range for stacked bars\n",
        "\n",
        "    fig.update_xaxes(title_text=\"Word (Key Source)\", row=1, col=1)\n",
        "    fig.update_xaxes(title_text=\"Word (Value Source)\", row=2, col=1)\n",
        "    fig.update_xaxes(title_text=\"Embedding Dimension\", row=3, col=1)\n",
        "\n",
        "    fig.update_layout(hoverlabel=dict(bgcolor=\"white\", font_size=12))\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "# === Example Execution Code (No Changes Needed Here) ===\n",
        "\n",
        "# === Example 1: Focusing on 'it' ===\n",
        "print(\"--- Running Example 1 ('it') ---\")\n",
        "# (Rest of the execution code remains the same as the previous working version)\n",
        "example = \"The animal didn't cross the street because it was too tired.\"\n",
        "words = example.split()\n",
        "focus_word_1 = \"it\"\n",
        "it_index = -1\n",
        "try:\n",
        "    it_index = words.index(focus_word_1)\n",
        "    print(f\"Found '{focus_word_1}' at index {it_index}.\")\n",
        "except ValueError:\n",
        "    print(f\"ERROR: Word '{focus_word_1}' not found in the sentence: '{example}'\")\n",
        "\n",
        "if it_index != -1:\n",
        "    try:\n",
        "        print(f\"Visualizing attention for word '{words[it_index]}' (index {it_index})...\")\n",
        "        fig = create_qkv_visualization_clearer(example, it_index)\n",
        "        fig.show()\n",
        "        print(f\"Example 1 visualization displayed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n!!! ERROR during visualization/display for Example 1 ('{focus_word_1}') !!!\")\n",
        "        print(f\"Error Type: {type(e).__name__}\")\n",
        "        print(f\"Error Details: {e}\")\n",
        "        print(\"Traceback:\")\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"Skipping visualization for Example 1 because word was not found.\")\n",
        "print(\"--- Finished Example 1 ---\")\n",
        "\n",
        "\n",
        "# === Example 2: Focusing on 'cat' ===\n",
        "# (Uncomment to run)\n",
        "print(\"\\n--- Running Example 2 ('cat') ---\")\n",
        "example2 = \"The big red dog chased the small cat.\"\n",
        "words2 = example2.split()\n",
        "focus_word_2 = \"cat\"\n",
        "cat_index = -1\n",
        "actual_word_found = None\n",
        "try:\n",
        "    actual_word_found = next((w for w in words2 if focus_word_2 in w), None)\n",
        "    if actual_word_found:\n",
        "        cat_index = words2.index(actual_word_found)\n",
        "        print(f\"Found '{actual_word_found}' (containing '{focus_word_2}') at index {cat_index}.\")\n",
        "    else:\n",
        "        print(f\"ERROR: Word containing '{focus_word_2}' not found in the sentence: '{example2}'\")\n",
        "except ValueError:\n",
        "     print(f\"ERROR: Value error occurred trying to find index for '{actual_word_found}' in sentence: '{example2}'\")\n",
        "except Exception as e:\n",
        "     print(f\"\\n!!! UNEXPECTED ERROR during word search for Example 2 ('{focus_word_2}') !!!\")\n",
        "     print(f\"Error Type: {type(e).__name__}\")\n",
        "     print(f\"Error Details: {e}\")\n",
        "     traceback.print_exc()\n",
        "\n",
        "if cat_index != -1 and actual_word_found:\n",
        "    try:\n",
        "        print(f\"Visualizing attention for word '{actual_word_found}' (index {cat_index})...\")\n",
        "        fig2 = create_qkv_visualization_clearer(example2, cat_index)\n",
        "        fig2.show()\n",
        "        print(f\"Example 2 visualization displayed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n!!! ERROR during visualization/display for Example 2 ('{actual_word_found}') !!!\")\n",
        "        print(f\"Error Type: {type(e).__name__}\")\n",
        "        print(f\"Error Details: {e}\")\n",
        "        print(\"Traceback:\")\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "     print(\"Skipping visualization for Example 2 because word was not found or index search failed.\")\n",
        "# print(\"--- Finished Example 2 ---\")"
      ],
      "metadata": {
        "id": "0SFsGXQ7a6Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Multi-Head Attention: Different Perspectives 🧐🤯🤓\n",
        "Transformers don't just calculate attention once. They use **Multi-Head Attention**.\n",
        "\n",
        "**The Idea:** Instead of having one set of Q, K, V weight matrices, Multi-Head Attention has multiple sets (e.g., 8 or 12 \"heads\"). Each head learns a *different* set of Wq, Wk, Wv matrices.\n",
        "\n",
        "1.  The input embedding (with positional encoding) is passed through each head independently.\n",
        "2.  Each head performs the self-attention calculation (QKV dot products, softmax, weighted sum) in parallel, potentially focusing on different types of relationships or aspects of the sequence.\n",
        "3.  The output vectors from all heads are concatenated together.\n",
        "4.  This concatenated vector is passed through one final linear layer (matrix multiplication) to produce the final output of the Multi-Head Attention block.\n",
        "\n",
        "> **Analogy: Expert Panel** 🧑‍🏫👩‍🔬👨‍🎨\n",
        "> Imagine analyzing a complex sentence. Instead of one person trying to understand everything, you have a panel of experts (attention heads):\n",
        "> * One expert focuses on grammatical dependencies (subject-verb, pronoun references).\n",
        "> * Another focuses on semantic similarity (synonyms, related concepts).\n",
        "> * Another might focus on positional relationships (\"the word before X\").\n",
        "> Each head provides its own contextual understanding based on its learned specialty. Combining their outputs gives a much richer, multi-faceted representation than a single attention calculation could.\n",
        "\n",
        "Multi-Head Attention allows the model to jointly attend to information from different representation subspaces at different positions.\n"
      ],
      "metadata": {
        "id": "bWmbZ5kJa1R6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Attention visualizer (expand to see code)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "from IPython.display import display, HTML, clear_output, Markdown\n",
        "import torch\n",
        "import ipywidgets as widgets\n",
        "import seaborn as sns\n",
        "\n",
        "def get_attention_patterns(model, tokenizer, text):\n",
        "    \"\"\"\n",
        "    Extract attention patterns from the model for visualization.\n",
        "    \"\"\"\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # Get model outputs with attention\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "\n",
        "    # Extract attention weights and convert to numpy\n",
        "    attention_weights = []\n",
        "    for layer_attentions in outputs.attentions:\n",
        "        layer_attentions = layer_attentions.squeeze(0)\n",
        "        layer_attentions = layer_attentions.cpu().numpy()\n",
        "        attention_weights.append(layer_attentions)\n",
        "\n",
        "    # Get tokens\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "    return attention_weights, tokens\n",
        "\n",
        "def create_attention_visualizer(model, tokenizer, examples=None):\n",
        "    # Create informative explanation of attention visualization\n",
        "    explanation = \"\"\"\n",
        "    <div style=\"background-color: #1e1e1e; padding: 20px; border-radius: 8px; margin-bottom: 20px; color: #e1e1e1;\">\n",
        "        <h3 style=\"color: #00b4d8; margin-top: 0;\">How to Read This Attention Heatmap</h3>\n",
        "\n",
        "        <p>This visualization shows how each token (word piece) pays attention to other tokens and itself when processing text:</p>\n",
        "\n",
        "        <ul style=\"margin-bottom: 15px;\">\n",
        "            <li><strong style=\"color: #00b4d8;\">Reading the Grid:</strong> Each row shows how much attention a token pays to previous tokens and itself (columns)</li>\n",
        "            <li><strong style=\"color: #00b4d8;\">Numbers & Colors:</strong> Darker blue and higher numbers (0-1) indicate stronger attention</li>\n",
        "            <li><strong style=\"color: #00b4d8;\">Blank Upper Area:</strong> Tokens can only attend to themselves and previous tokens, so the upper triangle is always blank</li>\n",
        "        </ul>\n",
        "\n",
        "        <p><strong style=\"color: #00b4d8;\">Example Pattern:</strong> In the sentence \"The cat sits\", you might see:</p>\n",
        "        <ul>\n",
        "            <li>\"cat\" paying strong attention to \"The\" (article-noun relationship)</li>\n",
        "            <li>\"sits\" paying attention to \"cat\" (subject-verb relationship)</li>\n",
        "            <li>Each token typically paying some attention to itself</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    if examples is None:\n",
        "        examples = {\n",
        "            \"Complex Relationship\": \"The dog chased its tail because it was wagging\",\n",
        "            \"Question-Answer\": \"Q: What is the capital of France? A: Paris\",\n",
        "            \"Local Grammar\": \"The red and blue car\",\n",
        "            \"Grammar Pattern\": \"The red and blue car drove fast\",\n",
        "            \"Completion\": \"The students studied hard for their final\",\n",
        "            \"Simple Example\": \"The cat sits on the mat\",\n",
        "            \"Comparison\": \"Although it was expensive, the quality was excellent\",\n",
        "        }\n",
        "\n",
        "    # Create widgets\n",
        "    text_input = widgets.Text(\n",
        "        value='',\n",
        "        placeholder='Enter custom text...',\n",
        "        description='Input:',\n",
        "        layout=widgets.Layout(width='80%')\n",
        "    )\n",
        "\n",
        "    examples_dropdown = widgets.Dropdown(\n",
        "        options=examples,\n",
        "        description='Examples:',\n",
        "        layout=widgets.Layout(width='80%')\n",
        "    )\n",
        "\n",
        "    layer_dropdown = widgets.Dropdown(\n",
        "        options=[(f'Layer {i}', i) for i in range(32)],\n",
        "        value=0,\n",
        "        description='Layer:',\n",
        "        layout=widgets.Layout(width='200px')\n",
        "    )\n",
        "\n",
        "    head_dropdown = widgets.Dropdown(\n",
        "        options=[(f'Head {i}', i) for i in range(32)],\n",
        "        value=0,\n",
        "        description='Head:',\n",
        "        layout=widgets.Layout(width='200px')\n",
        "    )\n",
        "\n",
        "    viz_output = widgets.Output()\n",
        "    rec_output = widgets.Output()\n",
        "\n",
        "    def update_display(change=None):\n",
        "        \"\"\"Update both visualization and recommendations\"\"\"\n",
        "        text = text_input.value if text_input.value else examples_dropdown.value\n",
        "        layer = layer_dropdown.value\n",
        "        head = head_dropdown.value\n",
        "\n",
        "        attention_weights, tokens = get_attention_patterns(model, tokenizer, text)\n",
        "\n",
        "        with rec_output:\n",
        "            clear_output(wait=True)\n",
        "            interesting_patterns = find_interesting_patterns(attention_weights, tokens)\n",
        "            print(\"\\nRecommended interesting patterns to explore:\")\n",
        "            for l, h, score, reason in interesting_patterns:\n",
        "                print(f\"Layer {l}, Head {h}: {reason} (score: {score:.2f})\")\n",
        "\n",
        "        with viz_output:\n",
        "            clear_output(wait=True)\n",
        "\n",
        "            n_tokens = len(tokens)\n",
        "            fig_size = max(6, n_tokens * 0.5)\n",
        "            plt.figure(figsize=(fig_size, fig_size))\n",
        "\n",
        "            # Create mask for upper triangle\n",
        "            mask = np.triu(np.ones_like(attention_weights[layer][head]), k=1)\n",
        "\n",
        "            # Plot heatmap with mask\n",
        "            sns.heatmap(attention_weights[layer][head],\n",
        "                       xticklabels=tokens,\n",
        "                       yticklabels=tokens,\n",
        "                       cmap='Blues',\n",
        "                       center=0.5,\n",
        "                       square=True,\n",
        "                       fmt='.2f',\n",
        "                       annot=True,\n",
        "                       mask=mask,  # Apply mask to hide upper triangle\n",
        "                       annot_kws={'size': 8},\n",
        "                       cbar_kws={'shrink': .8})\n",
        "\n",
        "            plt.title(f'Attention Pattern (Layer {layer}, Head {head})')\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            plt.yticks(rotation=0)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    # Connect callbacks\n",
        "    text_input.observe(update_display, names='value')\n",
        "    examples_dropdown.observe(update_display, names='value')\n",
        "    layer_dropdown.observe(update_display, names='value')\n",
        "    head_dropdown.observe(update_display, names='value')\n",
        "\n",
        "    # Create layout with dropdowns side by side\n",
        "    controls = widgets.VBox([\n",
        "        widgets.HTML(explanation),  # Add explanation at the top\n",
        "        examples_dropdown,\n",
        "        text_input,\n",
        "        widgets.HBox([layer_dropdown, head_dropdown])\n",
        "    ])\n",
        "\n",
        "    # Display everything\n",
        "    display(widgets.HTML(\"<h2>Attention Pattern Visualizer</h2>\"))\n",
        "    display(controls)\n",
        "    display(viz_output)\n",
        "    display(rec_output)\n",
        "\n",
        "    # Show initial visualization\n",
        "    update_display()\n",
        "\n",
        "#@title Function to find interesting patterns in attention heatmaps\n",
        "def find_interesting_patterns(attention_weights, tokens, top_k=5):\n",
        "    scores = []\n",
        "    n_layers = len(attention_weights)\n",
        "    n_heads = attention_weights[0].shape[0]\n",
        "\n",
        "    for layer in range(n_layers):\n",
        "        for head in range(n_heads):\n",
        "            matrix = attention_weights[layer][head]\n",
        "\n",
        "            # Look for patterns where first token gets consistent attention\n",
        "            first_token_pattern = np.mean(matrix[:, 0]) > 0.5\n",
        "\n",
        "            # Look for adjective-noun patterns (decreasing attention)\n",
        "            decreasing_pattern = np.all(np.diff(matrix.mean(axis=1)) < 0.1)\n",
        "\n",
        "            # Look for distributed attention in nouns\n",
        "            last_token_distribution = matrix[-1].std() < 0.3 and matrix[-1].mean() > 0.1\n",
        "\n",
        "            # Calculate linguistic pattern score\n",
        "            linguistic_score = (\n",
        "                first_token_pattern * 0.4 +\n",
        "                decreasing_pattern * 0.3 +\n",
        "                last_token_distribution * 0.3\n",
        "            )\n",
        "\n",
        "            # Determine pattern type and score\n",
        "            if linguistic_score > 0.6:\n",
        "                reason = \"Shows grammatical structure patterns\"\n",
        "                score = linguistic_score\n",
        "            elif first_token_pattern:\n",
        "                reason = \"Shows article-word relationships\"\n",
        "                score = linguistic_score\n",
        "            else:\n",
        "                # Calculate general interest metrics as fallback\n",
        "                attention_spread = len(matrix[matrix > 0.05]) / matrix.size\n",
        "                peak_contrast = np.max(matrix) - np.mean(matrix)\n",
        "                weighted_distances = np.mean(np.abs(np.arange(len(tokens))[:, None] - np.arange(len(tokens))) * matrix)\n",
        "\n",
        "                score = (\n",
        "                    attention_spread * 0.4 +\n",
        "                    peak_contrast * 0.3 +\n",
        "                    weighted_distances * 0.3\n",
        "                )\n",
        "\n",
        "                if attention_spread > 0.3:\n",
        "                    reason = \"Shows distributed attention\"\n",
        "                elif peak_contrast > 0.4:\n",
        "                    reason = \"Shows focused attention peaks\"\n",
        "                elif weighted_distances > len(tokens)/3:\n",
        "                    reason = \"Shows long-range connections\"\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            scores.append((layer, head, score, reason))\n",
        "\n",
        "    # Sort by score and return top_k unique patterns\n",
        "    scores.sort(key=lambda x: x[2], reverse=True)\n",
        "    seen_reasons = set()\n",
        "    filtered_scores = []\n",
        "    for score in scores:\n",
        "        if score[3] not in seen_reasons and len(filtered_scores) < top_k:\n",
        "            filtered_scores.append(score)\n",
        "            seen_reasons.add(score[3])\n",
        "\n",
        "    return filtered_scores\n",
        "\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
        "examples = {\n",
        "    \"Grammar Pattern\": \"The red and blue car drove fast\",\n",
        "    \"Complex Relationship\": \"The dog chased its tail because it was wagging\",\n",
        "    \"Question-Answer\": \"Q: What is the capital of France? A: Paris\",\n",
        "    \"Completion\": \"The students studied hard for their final\",\n",
        "    \"Simple Example\": \"The cat sits on the mat\",\n",
        "    \"Comparison\": \"Although it was expensive, the quality was excellent\",\n",
        "}\n",
        "\n",
        "create_attention_visualizer(model, tokenizer, examples=examples)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RYmQQS4KYoH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Core Concept #3: The Transformer Architecture\n",
        "\n",
        "Now that we understand positional encoding and the powerful attention mechanism, let's assemble these pieces into the full Transformer architecture. Transformers are typically built by stacking multiple identical layers, often referred to as **Transformer Blocks**\n",
        "\n",
        "There is already a fantastic and full-featured visualization for the Transformer: https://poloclub.github.io/transformer-explainer/."
      ],
      "metadata": {
        "id": "H6tuQPmVU8mI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 The Building Block: A Single Transformer Layer\n",
        "\n",
        "Think of a Transformer model (like BERT or GPT) as a skyscraper built from many identical floors. Each \"floor\" is a Transformer Block (or Layer) that processes the sequence representation passed up from the floor below. A standard Transformer block typically contains two main sub-layers:\n",
        "\n",
        "1.  **Multi-Head Self-Attention Sub-layer:**\n",
        "    *   This is where the core attention mechanism happens (as discussed in Core Concept #2).\n",
        "    *   Input tokens (with positional encodings) generate Queries, Keys, and Values.\n",
        "    *   Multiple \"heads\" calculate attention scores in parallel, allowing the model to focus on different types of relationships simultaneously.\n",
        "    *   The outputs of the heads are combined to produce an output representation for each token that incorporates context from the entire sequence.\n",
        "\n",
        "2.  **Position-wise Feed-Forward Network (FFN) Sub-layer:**\n",
        "    *   After the attention mechanism gathers context, this sub-layer processes each token's representation *independently*.\n",
        "    *   It's usually a simple two-layer fully-connected network (Linear -> ReLU Activation -> Linear).\n",
        "    *   Think of it as further \"thinking\" about the context-rich representation derived from the attention step for each position separately.\n",
        "    *   *Important:* The *same* FFN (same weights) is applied to each position, but it acts on each position's vector one by one.\n",
        "\n",
        "**Connecting the Sub-layers: Add & Norm**\n",
        "\n",
        "Crucially, after *each* of these two sub-layers, an **Add & Norm** step is applied:\n",
        "\n",
        "*   **Add (Residual Connection):** The input *to* the sub-layer (e.g., the input to the Multi-Head Attention) is added directly to the output *of* that sub-layer. This creates a \"shortcut\" or \"residual connection\".\n",
        "    *   *Why?* This simple addition makes it much easier to train very deep networks. It allows information to bypass a layer if needed and helps gradients flow backwards during training without vanishing.\n",
        "*   **Norm (Layer Normalization):** After the addition, Layer Normalization is applied. It rescales the values within each token's vector independently across the feature dimension.\n",
        "    *   *Why?* This helps stabilize the learning process, making the model less sensitive to the scale of parameters and gradients, often leading to faster convergence.\n",
        "\n",
        "So, the flow through one complete Transformer Block looks like this:\n",
        "`Input -> Multi-Head Attention -> Add & Norm -> Feed-Forward Network -> Add & Norm -> Output`"
      ],
      "metadata": {
        "id": "opwq5iU7MO2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Stacking Blocks: Encoders and Decoders\n",
        "\n",
        "These Transformer Blocks are stacked one on top of another to form the main components of the architecture: the Encoder and the Decoder.\n",
        "\n",
        "### The Encoder Stack\n",
        "\n",
        "*   **Purpose:** To read and \"understand\" the input sequence, creating a rich, context-aware representation.\n",
        "*   **Structure:** A stack of N identical Encoder Blocks (e.g., N=6 or 12).\n",
        "*   **Input:** The sequence of input embeddings (word embeddings + positional encodings).\n",
        "*   **Process:** The input flows through the stack. Each Encoder Block applies self-attention (allowing all input tokens to attend to each other) and feed-forward layers as described above. The output of one block becomes the input to the next.\n",
        "*   **Output:** A sequence of context-rich vectors, one for each input token, representing the \"meaning\" of the input sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "6t05Fb24MQc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Decoder Stack\n",
        "\n",
        "*   **Purpose:** To generate an output sequence (e.g., a translation, a summary, or the next word in a sentence), often conditioned on the Encoder's output.\n",
        "*   **Structure:** A stack of N identical Decoder Blocks.\n",
        "*   **Input:** Typically takes the previously generated output token embeddings (plus positional encodings) and the final output from the Encoder stack.\n",
        "*   **Process:** Decoder Blocks are slightly different from Encoder Blocks. They have *three* main sub-layers (each followed by Add & Norm):\n",
        "    1.  **Masked Multi-Head Self-Attention:** The Decoder performs self-attention on the sequence it has generated *so far*. The \"Masking\" is crucial: it prevents a position from attending to *future* positions. This is essential because during generation, the model should only use the words it has already produced, not the \"correct\" future words.\n",
        "    2.  **Multi-Head Cross-Attention:** This is where the Decoder interacts with the Encoder's output. The *Queries* come from the Decoder's Masked Self-Attention output, while the *Keys* and *Values* come from the final output of the *Encoder stack*. This allows the Decoder to focus on relevant parts of the *input* sequence while generating the *output* sequence. (e.g., when translating, focus on the relevant source words).\n",
        "    3.  **Position-wise Feed-Forward Network:** Identical in function to the FFN in the Encoder block, processing the output of the cross-attention step for each position.\n",
        "*   **Output:** After the final Decoder Block, a linear layer and a softmax function are typically used to predict the probabilities for the *next* token in the output sequence."
      ],
      "metadata": {
        "id": "TziVDodAUii_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plotly Visualization: Attention Masking Heatmap\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.subplots as sp # Import subplots\n",
        "\n",
        "# Create a dummy attention score matrix (e.g., for a sequence of 5 tokens)\n",
        "# These would normally be the result of Q dot K^T\n",
        "np.random.seed(42)\n",
        "seq_len_mask = 5\n",
        "attention_scores = np.random.randn(seq_len_mask, seq_len_mask)\n",
        "\n",
        "# Create the mask (lower triangular matrix including diagonal allows attention)\n",
        "# Mask value set for entries we want to *ignore* (upper triangle)\n",
        "mask_value = -1e9 # Use a large negative number for masking\n",
        "# np.triu -> Upper triangle. k=1 means exclude the diagonal.\n",
        "mask = np.triu(np.ones((seq_len_mask, seq_len_mask)) * mask_value, k=1)\n",
        "\n",
        "# Apply the mask - Add mask to scores. Where mask is 0, scores are unchanged.\n",
        "# Where mask is -1e9, scores become very small negative numbers.\n",
        "masked_scores = attention_scores + mask\n",
        "\n",
        "# --- Visualization ---\n",
        "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=(\"Raw Attention Scores\", \"Masked Attention Scores (before Softmax)\"))\n",
        "\n",
        "# Plot Raw Scores\n",
        "heatmap_raw = go.Heatmap(\n",
        "    z=attention_scores,\n",
        "    x=[f'Key {i+1}' for i in range(seq_len_mask)],\n",
        "    y=[f'Query {i+1}' for i in range(seq_len_mask)],\n",
        "    colorscale='viridis',\n",
        "    zmin=-3, zmax=3, # Consistent color range\n",
        "    text=np.round(attention_scores, 2),\n",
        "    texttemplate=\"%{text}\",\n",
        "    showscale=False,\n",
        "    name=\"Raw Scores\"\n",
        ")\n",
        "fig.add_trace(heatmap_raw, row=1, col=1)\n",
        "\n",
        "# Plot Masked Scores\n",
        "# Create text labels: show score where not masked, '-inf' where masked\n",
        "masked_text = np.full_like(masked_scores, \"\", dtype=object)\n",
        "not_masked_indices = (mask != mask_value)\n",
        "masked_indices = (mask == mask_value)\n",
        "masked_text[not_masked_indices] = np.round(masked_scores[not_masked_indices], 2)\n",
        "masked_text[masked_indices] = \"-inf\" # Indicate masked cells visually\n",
        "\n",
        "heatmap_masked = go.Heatmap(\n",
        "    # Use original scores for coloring non-masked, and a distinct value (NaN) for masked\n",
        "    z=np.where(masked_indices, np.nan, attention_scores),\n",
        "    x=[f'Key {i+1}' for i in range(seq_len_mask)],\n",
        "    y=[f'Query {i+1}' for i in range(seq_len_mask)],\n",
        "    colorscale='viridis', # Same scale for comparison\n",
        "    zmin=-3, zmax=3,\n",
        "    text=masked_text,\n",
        "    texttemplate=\"%{text}\",\n",
        "    hoverongaps=False, # Don't show hover info for masked cells\n",
        "    showscale=True,\n",
        "    colorbar_title=\"Score\",\n",
        "    name=\"Masked Scores\",\n",
        "    coloraxis=\"coloraxis\" # Link color axis if needed, though separate is fine here\n",
        ")\n",
        "# Explicitly set the color for NaN (masked) gaps\n",
        "heatmap_masked.update(xgap=1, ygap=1, connectgaps=False) # Add gaps visually\n",
        "fig.layout.coloraxis.colorbar.title = 'Score'\n",
        "fig.layout.coloraxis.colorscale = 'Viridis'\n",
        "fig.layout.coloraxis.cmin = -3\n",
        "fig.layout.coloraxis.cmax = 3\n",
        "\n",
        "\n",
        "fig.add_trace(heatmap_masked, row=1, col=2)\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=\"Masked Self-Attention: Preventing Future Peeking\",\n",
        "    height=400, width=850,\n",
        "    # Explicitly set background for masked cells if desired (e.g., light grey)\n",
        "    # This requires more complex setup or post-processing usually.\n",
        "    # Using NaN and connectgaps=False provides a visual distinction.\n",
        "    plot_bgcolor='white'\n",
        ")\n",
        "# Ensure y-axis is reversed to match matrix layout (Query 1 at top)\n",
        "fig.update_yaxes(autorange=\"reversed\")\n",
        "fig.show()\n",
        "\n",
        "# print(\"\\nExplanation:\") # Keep explanation outside the code cell if preferred\n",
        "# print(\"* Left Heatmap: Raw scores calculated by comparing each Query (row) to each Key (column).\")\n",
        "# print(\"* Right Heatmap: Mask applied. Scores in the upper right (Query i attending to Key j where j > i) are set to negative infinity ('-inf' text).\")\n",
        "# print(\"* Effect: After softmax, these '-inf' scores become 0, meaning a token cannot attend to subsequent tokens.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QaFDcrGOUteE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation:*\n",
        "*   *Left Heatmap:* Raw scores calculated by comparing each Query (row) to each Key (column).\n",
        "*   *Right Heatmap:* Mask applied. Scores in the upper right (where a Query tries to attend to a Key further down the sequence) are effectively set to negative infinity (visualized as gaps / \"-inf\" text).\n",
        "*   *Effect:* After the softmax function is applied to these masked scores, the masked positions will have a probability of zero, ensuring tokens only attend to themselves and previous tokens.\n",
        "\n",
        "**Visualization: Decoder Stack with Cross-Attention**"
      ],
      "metadata": {
        "id": "ghjfm4VsU-rt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Common Architectures & Use Cases\n",
        "\n",
        "Based on these components, three main types of Transformer architectures are prevalent:\n",
        "\n",
        "1.  **Encoder-Only Models (e.g., BERT, RoBERTa):**\n",
        "    *   Use only the Encoder stack.\n",
        "    *   Excellent at tasks requiring deep understanding of the input text.\n",
        "    *   **Applications:** Text classification, sentiment analysis, named entity recognition, question answering (where the answer is extracted from the context).\n",
        "\n",
        "2.  **Decoder-Only Models (e.g., GPT series, LLaMA, Gemini):**\n",
        "    *   Use only the Decoder stack (with its masked self-attention).\n",
        "    *   Excellent at generating coherent text following a prompt.\n",
        "    *   **Applications:** Text generation, autocompletion, chatbots, creative writing, summarization (as a generation task).\n",
        "\n",
        "3.  **Encoder-Decoder Models (e.g., Original Transformer \"Attention Is All You Need\", T5, BART):**\n",
        "    *   Use both stacks connected via the cross-attention mechanism.\n",
        "    *   Designed for sequence-to-sequence tasks where an input sequence needs to be transformed into an output sequence.\n",
        "    *   **Applications:** Machine translation, text summarization (as a transformation task), question answering (generating the answer)."
      ],
      "metadata": {
        "id": "1esVVQSyMYgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EpHYPbC4U9cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 What Makes Transformers So Powerful? (Summary)\n",
        "\n",
        "The architecture combines several key innovations:\n",
        "\n",
        "1.  **Parallelism**: Processes the entire sequence at once (within layers).\n",
        "2.  **Self-Attention**: Directly models relationships between words regardless of distance.\n",
        "3.  **Positional Encoding**: Preserves sequence order information despite parallel processing.\n",
        "4.  **Multi-Head Attention**: Captures different types of relationships simultaneously.\n",
        "5.  **Deep Architecture**: Stacks multiple layers (enabled by Add & Norm) for sophisticated processing.\n",
        "6.  **Contextualization**: Cross-attention (in Encoder-Decoder models) effectively links input and output sequences.\n",
        "\n",
        "This design allows Transformers to capture the complex patterns and long-range dependencies in data like human language, leading to their state-of-the-art performance on many tasks."
      ],
      "metadata": {
        "id": "whY1yCBbMmA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 4: Impact and Conclusion 🌟\n",
        "---\n",
        "The Transformer architecture has fundamentally changed the landscape of AI, especially in Natural Language Processing (NLP).\n",
        "\n",
        "**Key Advantages:**\n",
        "* **Parallelism:** Processes sequences in parallel, leading to significantly faster training times compared to RNNs on suitable hardware.\n",
        "* **Long-Range Dependencies:** Self-attention allows direct interaction between any two positions in the sequence, making it much better at capturing long-range context than traditional RNNs.\n",
        "* **State-of-the-Art Performance:** Transformers form the basis of models that have achieved top results across a vast range of NLP tasks, including:\n",
        "    * Machine Translation (e.g., Google Translate improvements)\n",
        "    * Text Summarization\n",
        "    * Question Answering\n",
        "    * Text Generation (e.g., GPT, Gemini)\n",
        "    * Language Understanding (e.g., BERT)\n",
        "* **Transfer Learning:** Pre-trained Transformer models (like BERT or GPT) can be fine-tuned on specific downstream tasks with relatively small amounts of data, achieving excellent performance.\n",
        "* **Beyond NLP:** The core ideas, particularly attention, have been successfully adapted to other domains like computer vision (Vision Transformers - ViT), audio processing, and even biology.\n",
        "\n",
        "**Conclusion:**\n",
        "We've seen how Transformers move beyond the limitations of sequential processing by leveraging parallel computation and the powerful **Self-Attention** mechanism. By allowing every part of the input to directly attend to every other part, and using techniques like **Positional Encoding** to retain order information and **Multi-Head Attention** to capture diverse relationships, Transformers build deep contextual understanding. The **Encoder-Decoder** structure provides a flexible framework for various sequence-to-sequence tasks.\n",
        "\n",
        "While the internal workings involve complex linear algebra and learned parameters, the core concepts of parallel processing and attention provide an intuition for why these models are so effective. They represent a significant step forward in our ability to build AI that can understand and generate complex, sequential data like human language.\n",
        "\n",
        "This concludes our planned sessions on core AI architectures. We've journeyed from individual neurons to MLPs, CNNs, RNNs, and now Transformers, seeing how different ways of connecting these components lead to specialized capabilities.\n"
      ],
      "metadata": {
        "id": "LLu6KlWIfyhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Glossary\n",
        "---\n",
        "* **Transformer:** A neural network architecture based on self-attention mechanisms, processing sequences in parallel.\n",
        "* **Self-Attention:** A mechanism allowing inputs to interact with each other and compute weights (attention scores) indicating their relevance to each other.\n",
        "* **Query (Q):** In attention, a representation of the current token used to score against keys.\n",
        "* **Key (K):** In attention, a representation of a token used to be matched against queries.\n",
        "* **Value (V):** In attention, a representation of a token's content, weighted by attention scores to produce the output.\n",
        "* **Multi-Head Attention:** Performing the attention mechanism multiple times in parallel with different learned linear projections (different sets of Q, K, V) and concatenating the results.\n",
        "* **Positional Encoding:** Information added to input embeddings to provide the model with knowledge of the token's position in the sequence.\n",
        "* **Encoder (Transformer):** The part of the Transformer that processes the input sequence to build contextualized representations. Contains self-attention and feed-forward layers.\n",
        "* **Decoder (Transformer):** The part of the Transformer that generates the output sequence, using masked self-attention and cross-attention to the encoder output.\n",
        "* **Masked Self-Attention:** Self-attention where future positions in the sequence are masked out, typically used in decoders during training to prevent looking ahead.\n",
        "* **Cross-Attention:** Attention mechanism where queries come from one sequence (e.g., decoder) and keys/values come from another (e.g., encoder output).\n",
        "* **Add & Norm:** A layer combining a residual (shortcut) connection with layer normalization, used to stabilize training in deep networks.\n",
        "* **Layer Normalization:** Normalizes activations across the feature dimension for each sequence element independently.\n",
        "* **Position-wise Feed-Forward Network (FFN):** An MLP applied independently to each position in the sequence within a Transformer layer.\n",
        "* **Softmax:** A function that converts a vector of raw scores into a probability distribution (values between 0 and 1 that sum to 1).\n",
        "* **Embedding:** A learned vector representation of a discrete item (like a word or token).\n"
      ],
      "metadata": {
        "id": "S4REIHSSgFeQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ndaJLmPNgK93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Resources\n",
        "---\n",
        "* **Original Paper:** Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). [Attention is all you need](https://arxiv.org/abs/1706.03762). Advances in neural information processing systems, 30.\n",
        "* **Illustrated Transformer:** Jay Alammar's blog post provides excellent visualizations and explanations: [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "* **Illustrated BERT:** Jay Alammar's explanation of BERT (an Encoder-only Transformer): [The Illustrated BERT, ELMo, and co.](http://jalammar.github.io/illustrated-bert/)\n",
        "* **Illustrated GPT-2:** Jay Alammar's explanation of GPT-2 (a Decoder-only Transformer): [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)\n",
        "* **Hugging Face Blog/Course:** Hugging Face provides extensive resources and tutorials on Transformers: [Hugging Face Blog](https://huggingface.co/blog), [Hugging Face Course](https://huggingface.co/course)\n"
      ],
      "metadata": {
        "id": "XhjHE1I2gLc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "O-ToiiyuhWob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "License Information\n",
        "---\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2025 Pate Motter\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, srublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ],
      "metadata": {
        "id": "nvvFxvicgOaJ"
      }
    }
  ]
}